{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16150bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_15260\\3285755810.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"score\", group_keys=False).apply(lambda x: x.sample(n=min(50000, len(x)), random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84412edd9b804df7bc0c4e1d6f1b199a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95f0a48f7124a90a7e98a96208f7404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_15260\\3285755810.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37500/37500 30:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.138300</td>\n",
       "      <td>1.168814</td>\n",
       "      <td>0.503240</td>\n",
       "      <td>0.498717</td>\n",
       "      <td>0.498717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.108300</td>\n",
       "      <td>1.157901</td>\n",
       "      <td>0.510680</td>\n",
       "      <td>0.502485</td>\n",
       "      <td>0.502485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.050900</td>\n",
       "      <td>1.180473</td>\n",
       "      <td>0.509040</td>\n",
       "      <td>0.505794</td>\n",
       "      <td>0.505794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1804733276367188, 'eval_accuracy': 0.50904, 'eval_f1_macro': 0.5057936054958516, 'eval_f1_weighted': 0.5057936054958515, 'eval_runtime': 19.6878, 'eval_samples_per_second': 2539.644, 'eval_steps_per_second': 79.389, 'epoch': 3.0}\n",
      "\n",
      "Classification report (DistilBERT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5957    0.5900    0.5928     10000\n",
      "           2     0.4180    0.4254    0.4217     10000\n",
      "           3     0.3928    0.3731    0.3827     10000\n",
      "           4     0.4818    0.4343    0.4568     10000\n",
      "           5     0.6334    0.7224    0.6750     10000\n",
      "\n",
      "    accuracy                         0.5090     50000\n",
      "   macro avg     0.5043    0.5090    0.5058     50000\n",
      "weighted avg     0.5043    0.5090    0.5058     50000\n",
      "\n",
      "Confusion matrix:\n",
      " [[5900 2548  939  246  367]\n",
      " [2335 4254 2175  770  466]\n",
      " [1036 2404 3731 1950  879]\n",
      " [ 383  764 2040 4343 2470]\n",
      " [ 250  207  613 1706 7224]]\n"
     ]
    }
   ],
   "source": [
    "# pip install -U transformers datasets accelerate evaluate scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate, torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(\"TopGamesDataClean.csv\", usecols=[\"content\", \"score\"])\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df = df[df[\"score\"].between(1,5)].dropna(subset=[\"content\"])\n",
    "df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "df = df[df[\"content\"] != \"\"]\n",
    "\n",
    "# (Optional) Subsample for speed\n",
    "df = df.groupby(\"score\", group_keys=False).apply(lambda x: x.sample(n=min(50000, len(x)), random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# labels must be 0..4 for HF models\n",
    "label_map = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "df[\"label\"] = df[\"score\"].map(label_map).astype(int)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df[[\"content\", \"label\"]],\n",
    "    test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# 2) Tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"content\"], truncation=True, max_length=256)\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "test_tok  = test_ds.map(tokenize,  batched=True, remove_columns=[\"content\"])\n",
    "\n",
    "# 3) Model\n",
    "num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# 4) Training setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"distilbert-5cls\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5) Evaluate\n",
    "eval_res = trainer.evaluate()\n",
    "print(eval_res)\n",
    "\n",
    "# 6) Predict on test set (labels back to 1..5)\n",
    "preds = np.argmax(trainer.predict(test_tok).predictions, axis=1)\n",
    "inv_label_map = {v:k for k,v in label_map.items()}\n",
    "pred_scores = np.vectorize(inv_label_map.get)(preds)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification report (DistilBERT):\\n\",\n",
    "      classification_report(np.vectorize(inv_label_map.get)(test_df[\"label\"].values),\n",
    "                            pred_scores, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(np.vectorize(inv_label_map.get)(test_df[\"label\"].values), pred_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5a4e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a0e2ae7f1f4c7da6f7345907a84862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1620599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ed650fb7554cc5a76d89331879adfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/405150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_15260\\2407354302.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='303864' max='303864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [303864/303864 4:00:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.847700</td>\n",
       "      <td>0.847515</td>\n",
       "      <td>0.690295</td>\n",
       "      <td>0.468372</td>\n",
       "      <td>0.647658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.796200</td>\n",
       "      <td>0.843503</td>\n",
       "      <td>0.691351</td>\n",
       "      <td>0.484785</td>\n",
       "      <td>0.660098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>0.855507</td>\n",
       "      <td>0.690406</td>\n",
       "      <td>0.491885</td>\n",
       "      <td>0.661771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8555066585540771, 'eval_accuracy': 0.690406022460817, 'eval_f1_macro': 0.49188483010881556, 'eval_f1_weighted': 0.6617705737910411, 'eval_runtime': 161.9609, 'eval_samples_per_second': 2501.53, 'eval_steps_per_second': 78.173, 'epoch': 3.0}\n",
      "\n",
      "Classification report (DistilBERT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6785    0.8321    0.7475     93654\n",
      "           2     0.4511    0.1036    0.1685     27683\n",
      "           3     0.3499    0.2924    0.3186     37901\n",
      "           4     0.4654    0.3266    0.3839     55666\n",
      "           5     0.7958    0.8918    0.8411    190246\n",
      "\n",
      "    accuracy                         0.6904    405150\n",
      "   macro avg     0.5481    0.4893    0.4919    405150\n",
      "weighted avg     0.6580    0.6904    0.6618    405150\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 77929   1671   5568   1756   6730]\n",
      " [ 14762   2867   5222   2001   2831]\n",
      " [ 10555   1302  11082   6860   8102]\n",
      " [  5006    358   6245  18183  25874]\n",
      " [  6610    158   3554  10267 169657]]\n"
     ]
    }
   ],
   "source": [
    "# pip install -U transformers datasets accelerate evaluate scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate, torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(\"TopGamesDataClean.csv\", usecols=[\"content\", \"score\"])\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df = df[df[\"score\"].between(1,5)].dropna(subset=[\"content\"])\n",
    "df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "df = df[df[\"content\"] != \"\"]\n",
    "\n",
    "# (Optional) Subsample for speed\n",
    "# df = df.groupby(\"score\", group_keys=False).apply(lambda x: x.sample(n=min(50000, len(x)), random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# labels must be 0..4 for HF models\n",
    "label_map = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "df[\"label\"] = df[\"score\"].map(label_map).astype(int)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df[[\"content\", \"label\"]],\n",
    "    test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# 2) Tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"content\"], truncation=True, max_length=256)\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "test_tok  = test_ds.map(tokenize,  batched=True, remove_columns=[\"content\"])\n",
    "\n",
    "# 3) Model\n",
    "num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# 4) Training setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"distilbert-5cls\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5) Evaluate\n",
    "eval_res = trainer.evaluate()\n",
    "print(eval_res)\n",
    "\n",
    "# 6) Predict on test set (labels back to 1..5)\n",
    "preds = np.argmax(trainer.predict(test_tok).predictions, axis=1)\n",
    "inv_label_map = {v:k for k,v in label_map.items()}\n",
    "pred_scores = np.vectorize(inv_label_map.get)(preds)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification report (DistilBERT):\\n\",\n",
    "      classification_report(np.vectorize(inv_label_map.get)(test_df[\"label\"].values),\n",
    "                            pred_scores, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(np.vectorize(inv_label_map.get)(test_df[\"label\"].values), pred_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56f68c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6057e468c96542108e9e28105a77ee77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09d65a6898440e5ab6d9ce1bfa35c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_15260\\1402960713.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10779' max='10779' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10779/10779 08:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.087700</td>\n",
       "      <td>1.058318</td>\n",
       "      <td>0.556986</td>\n",
       "      <td>0.487351</td>\n",
       "      <td>0.528486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>1.046454</td>\n",
       "      <td>0.567492</td>\n",
       "      <td>0.509907</td>\n",
       "      <td>0.548277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>1.074016</td>\n",
       "      <td>0.565057</td>\n",
       "      <td>0.518349</td>\n",
       "      <td>0.554908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0740162134170532, 'eval_accuracy': 0.5650570553854718, 'eval_f1_macro': 0.5183490056698038, 'eval_f1_weighted': 0.5549080762312745, 'eval_runtime': 5.8916, 'eval_samples_per_second': 2439.407, 'eval_steps_per_second': 76.38, 'epoch': 3.0}\n",
      "\n",
      "Classification report (DistilBERT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6332    0.7562    0.6893      3331\n",
      "           2     0.4129    0.1834    0.2540      1821\n",
      "           3     0.3940    0.4520    0.4210      2427\n",
      "           4     0.5046    0.5525    0.5275      3247\n",
      "           5     0.7323    0.6703    0.6999      3546\n",
      "\n",
      "    accuracy                         0.5651     14372\n",
      "   macro avg     0.5354    0.5229    0.5183     14372\n",
      "weighted avg     0.5603    0.5651    0.5549     14372\n",
      "\n",
      "Confusion matrix:\n",
      " [[2519  229  395  145   43]\n",
      " [ 757  334  505  190   35]\n",
      " [ 444  165 1097  588  133]\n",
      " [ 152   66  577 1794  658]\n",
      " [ 106   15  210  838 2377]]\n"
     ]
    }
   ],
   "source": [
    "# pip install -U transformers datasets accelerate evaluate scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate, torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(\"TopGamesDataClean.csv\", usecols=[\"content\", \"score\", 'game_name'])\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df = df[df[\"score\"].between(1,5)].dropna(subset=[\"content\"])\n",
    "df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "df = df[df[\"content\"] != \"\"]\n",
    "df = df[df[\"game_name\"] == \"Among Us\"]\n",
    "\n",
    "# (Optional) Subsample for speed\n",
    "# df = df.groupby(\"score\", group_keys=False).apply(lambda x: x.sample(n=min(50000, len(x)), random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# labels must be 0..4 for HF models\n",
    "label_map = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "df[\"label\"] = df[\"score\"].map(label_map).astype(int)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df[[\"content\", \"label\"]],\n",
    "    test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# 2) Tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"content\"], truncation=True, max_length=256)\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "test_tok  = test_ds.map(tokenize,  batched=True, remove_columns=[\"content\"])\n",
    "\n",
    "# 3) Model\n",
    "num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# 4) Training setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"distilbert-5cls\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5) Evaluate\n",
    "eval_res = trainer.evaluate()\n",
    "print(eval_res)\n",
    "\n",
    "# 6) Predict on test set (labels back to 1..5)\n",
    "preds = np.argmax(trainer.predict(test_tok).predictions, axis=1)\n",
    "inv_label_map = {v:k for k,v in label_map.items()}\n",
    "pred_scores = np.vectorize(inv_label_map.get)(preds)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification report (DistilBERT):\\n\",\n",
    "      classification_report(np.vectorize(inv_label_map.get)(test_df[\"label\"].values),\n",
    "                            pred_scores, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(np.vectorize(inv_label_map.get)(test_df[\"label\"].values), pred_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077bd191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_15260\\2380956854.py:105: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5394' max='5394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5394/5394 02:12, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.196600</td>\n",
       "      <td>1.165655</td>\n",
       "      <td>0.513707</td>\n",
       "      <td>0.422588</td>\n",
       "      <td>0.473560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.114100</td>\n",
       "      <td>1.119280</td>\n",
       "      <td>0.533120</td>\n",
       "      <td>0.448753</td>\n",
       "      <td>0.495597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.029200</td>\n",
       "      <td>1.104034</td>\n",
       "      <td>0.541748</td>\n",
       "      <td>0.482531</td>\n",
       "      <td>0.520431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>1.096651</td>\n",
       "      <td>0.542861</td>\n",
       "      <td>0.494399</td>\n",
       "      <td>0.529183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>1.106020</td>\n",
       "      <td>0.544949</td>\n",
       "      <td>0.496125</td>\n",
       "      <td>0.531706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>1.113924</td>\n",
       "      <td>0.544322</td>\n",
       "      <td>0.493440</td>\n",
       "      <td>0.529268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1060199737548828, 'eval_accuracy': 0.5449485109935986, 'eval_f1_macro': 0.4961245010818229, 'eval_f1_weighted': 0.5317055859078008, 'eval_runtime': 1.5915, 'eval_samples_per_second': 9030.576, 'eval_steps_per_second': 71.003, 'epoch': 6.0}\n",
      "\n",
      "Classification report (BiLSTM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6070    0.7550    0.6730      3331\n",
      "           2     0.3837    0.1840    0.2487      1821\n",
      "           3     0.3856    0.3869    0.3863      2427\n",
      "           4     0.4891    0.5057    0.4973      3247\n",
      "           5     0.6737    0.6771    0.6754      3546\n",
      "\n",
      "    accuracy                         0.5449     14372\n",
      "   macro avg     0.5078    0.5017    0.4961     14372\n",
      "weighted avg     0.5312    0.5449    0.5317     14372\n",
      "\n",
      "Confusion matrix:\n",
      " [[2515  239  351  145   81]\n",
      " [ 785  335  429  193   79]\n",
      " [ 499  204  939  586  199]\n",
      " [ 217   69  515 1642  804]\n",
      " [ 127   26  201  791 2401]]\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data\n",
    "df = pd.read_csv(\"TopGamesDataClean.csv\", usecols=[\"content\", \"score\", 'game_name'])\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df = df[df[\"score\"].between(1,5)].dropna(subset=[\"content\"])\n",
    "df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "df = df[df[\"content\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c2792d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167aa1ec0074488c9f46e1e5082d60bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/2025749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f91c9fb98e4bf89e7696dffed378cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/2025749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd2d33ad161496e897e8599664eddff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1823174 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc48bbd87d4a4dfd815ab48aa186384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/202575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b326bb95c0204395b77e9226f88e9cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2025749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_15260\\678802646.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-11-08 10:36:27,225] A new study created in memory with name: no-name-e1e1ffb6-8248-4944-a096-0fe99ee56faa\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='455794' max='455794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [455794/455794 5:59:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.822700</td>\n",
       "      <td>0.853137</td>\n",
       "      <td>0.690006</td>\n",
       "      <td>0.481092</td>\n",
       "      <td>0.654613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.763100</td>\n",
       "      <td>0.847166</td>\n",
       "      <td>0.692242</td>\n",
       "      <td>0.486981</td>\n",
       "      <td>0.660335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-08 16:36:14,551] Trial 0 finished with value: 0.48698108228640036 and parameters: {'learning_rate': 1.4134313218448877e-05, 'weight_decay': 0.07301787569610085, 'warmup_ratio': 0.03492832681816802, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'num_train_epochs': 2, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 0.48698108228640036.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='455794' max='455794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [455794/455794 7:11:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.856723</td>\n",
       "      <td>0.689290</td>\n",
       "      <td>0.478842</td>\n",
       "      <td>0.653014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.764700</td>\n",
       "      <td>0.845698</td>\n",
       "      <td>0.692790</td>\n",
       "      <td>0.485165</td>\n",
       "      <td>0.659493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-08 23:47:30,678] Trial 1 finished with value: 0.48516503018046403 and parameters: {'learning_rate': 1.7391409469426973e-05, 'weight_decay': 0.08885625355821286, 'warmup_ratio': 0.09255632508281356, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'num_train_epochs': 2, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 0.48698108228640036.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='284875' max='284875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [284875/284875 5:03:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.849586</td>\n",
       "      <td>0.689231</td>\n",
       "      <td>0.477678</td>\n",
       "      <td>0.651525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.830900</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>0.690702</td>\n",
       "      <td>0.479559</td>\n",
       "      <td>0.657236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.780200</td>\n",
       "      <td>0.842097</td>\n",
       "      <td>0.692188</td>\n",
       "      <td>0.482799</td>\n",
       "      <td>0.657738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.784200</td>\n",
       "      <td>0.854989</td>\n",
       "      <td>0.690579</td>\n",
       "      <td>0.485278</td>\n",
       "      <td>0.658273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.867929</td>\n",
       "      <td>0.687074</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.659659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 04:50:40,702] Trial 2 finished with value: 0.4902997037046825 and parameters: {'learning_rate': 1.2910933245630052e-05, 'weight_decay': 0.061391765485448224, 'warmup_ratio': 0.017944562881385774, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'num_train_epochs': 5, 'lr_scheduler_type': 'linear'}. Best is trial 2 with value: 0.4902997037046825.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='455796' max='455796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [455796/455796 6:03:37, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.854800</td>\n",
       "      <td>0.863915</td>\n",
       "      <td>0.684438</td>\n",
       "      <td>0.459417</td>\n",
       "      <td>0.639928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.852400</td>\n",
       "      <td>0.846815</td>\n",
       "      <td>0.689567</td>\n",
       "      <td>0.475092</td>\n",
       "      <td>0.654369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>0.844451</td>\n",
       "      <td>0.691507</td>\n",
       "      <td>0.482349</td>\n",
       "      <td>0.657098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>0.846790</td>\n",
       "      <td>0.690752</td>\n",
       "      <td>0.484192</td>\n",
       "      <td>0.658261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 10:54:19,085] Trial 3 finished with value: 0.4841920814542872 and parameters: {'learning_rate': 5.341368166342143e-06, 'weight_decay': 0.03944224903711979, 'warmup_ratio': 0.1949091135813038, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 64, 'num_train_epochs': 4, 'lr_scheduler_type': 'linear'}. Best is trial 2 with value: 0.4902997037046825.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='569745' max='569745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [569745/569745 9:09:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.855100</td>\n",
       "      <td>0.867584</td>\n",
       "      <td>0.682750</td>\n",
       "      <td>0.451493</td>\n",
       "      <td>0.634316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.849500</td>\n",
       "      <td>0.845510</td>\n",
       "      <td>0.689917</td>\n",
       "      <td>0.475682</td>\n",
       "      <td>0.655322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.843464</td>\n",
       "      <td>0.691635</td>\n",
       "      <td>0.481714</td>\n",
       "      <td>0.656911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.833800</td>\n",
       "      <td>0.845120</td>\n",
       "      <td>0.691996</td>\n",
       "      <td>0.482252</td>\n",
       "      <td>0.656934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.810600</td>\n",
       "      <td>0.854031</td>\n",
       "      <td>0.690544</td>\n",
       "      <td>0.489957</td>\n",
       "      <td>0.660442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 20:03:48,466] Trial 4 finished with value: 0.48995678847554547 and parameters: {'learning_rate': 6.7251172863520765e-06, 'weight_decay': 0.10314131354902362, 'warmup_ratio': 0.19993983250896374, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 64, 'num_train_epochs': 5, 'lr_scheduler_type': 'linear'}. Best is trial 2 with value: 0.4902997037046825.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='455796' max='455796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [455796/455796 8:47:49, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.857433</td>\n",
       "      <td>0.685662</td>\n",
       "      <td>0.458566</td>\n",
       "      <td>0.639838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.842200</td>\n",
       "      <td>0.843186</td>\n",
       "      <td>0.690352</td>\n",
       "      <td>0.478604</td>\n",
       "      <td>0.657521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.791400</td>\n",
       "      <td>0.841802</td>\n",
       "      <td>0.692682</td>\n",
       "      <td>0.486372</td>\n",
       "      <td>0.659881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.805900</td>\n",
       "      <td>0.855172</td>\n",
       "      <td>0.690421</td>\n",
       "      <td>0.490363</td>\n",
       "      <td>0.661061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 04:51:39,736] Trial 5 finished with value: 0.4903625701541766 and parameters: {'learning_rate': 1.2045308108537165e-05, 'weight_decay': 0.09092300989431466, 'warmup_ratio': 0.17294505407027128, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 64, 'num_train_epochs': 4, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 5 with value: 0.4903625701541766.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='227897' max='683691' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [227897/683691 4:41:18 < 9:22:36, 13.50 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.838400</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.687054</td>\n",
       "      <td>0.464504</td>\n",
       "      <td>0.644536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 09:32:58,920] Trial 6 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='341847' max='341847' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [341847/341847 7:11:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>0.854955</td>\n",
       "      <td>0.686926</td>\n",
       "      <td>0.462427</td>\n",
       "      <td>0.642847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.835100</td>\n",
       "      <td>0.841503</td>\n",
       "      <td>0.691650</td>\n",
       "      <td>0.480074</td>\n",
       "      <td>0.657581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>0.843729</td>\n",
       "      <td>0.692163</td>\n",
       "      <td>0.485661</td>\n",
       "      <td>0.659388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 16:44:35,800] Trial 7 finished with value: 0.4856611075241357 and parameters: {'learning_rate': 1.0653057689011208e-05, 'weight_decay': 0.10257929465774332, 'warmup_ratio': 0.18434172073395952, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 32, 'num_train_epochs': 3, 'lr_scheduler_type': 'cosine'}. Best is trial 5 with value: 0.4903625701541766.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='227897' max='911588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [227897/911588 3:54:39 < 11:43:59, 16.19 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.857500</td>\n",
       "      <td>0.868919</td>\n",
       "      <td>0.685218</td>\n",
       "      <td>0.460440</td>\n",
       "      <td>0.641652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 20:39:15,929] Trial 8 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='113950' max='227900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [113950/227900 2:14:23 < 2:14:23, 14.13 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.847935</td>\n",
       "      <td>0.690218</td>\n",
       "      <td>0.478544</td>\n",
       "      <td>0.652551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.827800</td>\n",
       "      <td>0.839448</td>\n",
       "      <td>0.692099</td>\n",
       "      <td>0.479480</td>\n",
       "      <td>0.657571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 22:53:39,641] Trial 9 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='113949' max='569745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [113949/569745 1:31:24 < 6:05:40, 20.77 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.867300</td>\n",
       "      <td>0.875796</td>\n",
       "      <td>0.679457</td>\n",
       "      <td>0.434207</td>\n",
       "      <td>0.623365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 00:25:05,778] Trial 10 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56975' max='284875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56975/284875 1:00:05 < 4:00:21, 15.80 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.857893</td>\n",
       "      <td>0.686778</td>\n",
       "      <td>0.467014</td>\n",
       "      <td>0.644683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 01:25:12,211] Trial 11 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56975' max='284875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56975/284875 1:00:12 < 4:00:50, 15.77 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.832100</td>\n",
       "      <td>0.852395</td>\n",
       "      <td>0.688427</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.650059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 02:25:25,897] Trial 12 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56975' max='227900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56975/227900 1:00:16 < 3:00:48, 15.75 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.854825</td>\n",
       "      <td>0.687029</td>\n",
       "      <td>0.464117</td>\n",
       "      <td>0.642498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 03:25:43,294] Trial 13 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170925' max='170925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170925/170925 3:00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.827300</td>\n",
       "      <td>0.848360</td>\n",
       "      <td>0.689636</td>\n",
       "      <td>0.480726</td>\n",
       "      <td>0.654070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.826400</td>\n",
       "      <td>0.841597</td>\n",
       "      <td>0.691048</td>\n",
       "      <td>0.480021</td>\n",
       "      <td>0.657045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.772100</td>\n",
       "      <td>0.841950</td>\n",
       "      <td>0.692193</td>\n",
       "      <td>0.485454</td>\n",
       "      <td>0.658863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 06:26:36,454] Trial 14 finished with value: 0.4854538665412809 and parameters: {'learning_rate': 1.2638607703581072e-05, 'weight_decay': 0.061215082657034826, 'warmup_ratio': 0.007422601170802312, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'num_train_epochs': 3, 'lr_scheduler_type': 'linear'}. Best is trial 5 with value: 0.4903625701541766.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: BestRun(run_id='5', objective=0.4903625701541766, hyperparameters={'learning_rate': 1.2045308108537165e-05, 'weight_decay': 0.09092300989431466, 'warmup_ratio': 0.17294505407027128, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 64, 'num_train_epochs': 4, 'lr_scheduler_type': 'cosine_with_restarts'}, run_summary=None)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transformers.training_args.TrainingArguments() got multiple values for keyword argument 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 123\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# 3) Retrain on 100% of the data with best HPs\u001b[39;00m\n\u001b[0;32m    122\u001b[0m best_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbase_args\u001b[38;5;241m.\u001b[39mto_dict(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest\u001b[38;5;241m.\u001b[39mhyperparameters}\n\u001b[1;32m--> 123\u001b[0m final_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_args,\n\u001b[0;32m    125\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-5cls-final\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    126\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,      \u001b[38;5;66;03m# training on all data; no holdout to pick \"best\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39mIntervalStrategy\u001b[38;5;241m.\u001b[39mNO, \u001b[38;5;66;03m# disable eval during training\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39mIntervalStrategy\u001b[38;5;241m.\u001b[39mEPOCH,\n\u001b[0;32m    129\u001b[0m )\n\u001b[0;32m    131\u001b[0m final_trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    132\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[0;32m    133\u001b[0m     args\u001b[38;5;241m=\u001b[39mfinal_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m final_trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mTypeError\u001b[0m: transformers.training_args.TrainingArguments() got multiple values for keyword argument 'output_dir'"
     ]
    }
   ],
   "source": [
    "# ---- CONTINUES FROM YOUR df ----\n",
    "# df already cleaned and contains columns: content (str), score (1..5)\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(\"TopGamesDataClean.csv\", usecols=[\"content\", \"score\", 'game_name'])\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df = df[df[\"score\"].between(1,5)].dropna(subset=[\"content\"])\n",
    "df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "df = df[df[\"content\"] != \"\"]\n",
    "\n",
    "import numpy as np, torch, evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# 0) Labels 1..5 -> 0..4\n",
    "label_map = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "df = df[df[\"score\"].between(1,5)]\n",
    "df[\"label\"] = df[\"score\"].map(label_map).astype(int)\n",
    "\n",
    "# 1) Tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "num_labels = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"content\"], truncation=True, max_length=256)\n",
    "\n",
    "# === SOLUSI 1: class_encode_column + stratified split, lalu tokenisasi ===\n",
    "# a) Buat Dataset & ubah 'label' menjadi ClassLabel\n",
    "full_ds = Dataset.from_pandas(df[[\"content\",\"label\"]].reset_index(drop=True))\n",
    "full_ds = full_ds.class_encode_column(\"label\")  # penting untuk stratify_by_column\n",
    "\n",
    "# b) Split dengan stratifikasi\n",
    "splits    = full_ds.train_test_split(test_size=0.1, stratify_by_column=\"label\", seed=42)\n",
    "train_all = splits[\"train\"]\n",
    "val_small = splits[\"test\"]\n",
    "\n",
    "# c) Tokenisasi SETELAH split\n",
    "train_all = train_all.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "val_small = val_small.map(tokenize,  batched=True, remove_columns=[\"content\"])\n",
    "\n",
    "# d) (Untuk pelatihan akhir 100% data) tokenisasi seluruh dataset juga\n",
    "full_tok = full_ds.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# 2) Hyperparameter tuning (Optuna backend)\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_f1_macro\"]\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-6, 5e-5, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.2),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.2),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [16, 32, 64]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\"]),\n",
    "    }\n",
    "\n",
    "base_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-5cls\",\n",
    "    eval_strategy=IntervalStrategy.EPOCH,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,                 # overridden by search\n",
    "    per_device_train_batch_size=16,     # overridden\n",
    "    per_device_eval_batch_size=32,      # overridden\n",
    "    num_train_epochs=3,                 # overridden\n",
    "    weight_decay=0.01,                  # overridden\n",
    "    warmup_ratio=0.0,                   # overridden\n",
    "    lr_scheduler_type=\"linear\",         # overridden\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=base_args,\n",
    "    train_dataset=train_all,\n",
    "    eval_dataset=val_small,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)],\n",
    ")\n",
    "\n",
    "best = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=compute_objective,\n",
    "    n_trials=15,\n",
    ")\n",
    "print(\"Best trial:\", best)\n",
    "\n",
    "# 3) Retrain on 100% of the data with best HPs\n",
    "best_args = {**base_args.to_dict(), **best.hyperparameters}\n",
    "final_args = TrainingArguments(\n",
    "    **best_args,\n",
    "    output_dir=\"distilbert-5cls-final\",\n",
    "    load_best_model_at_end=False,      # training on all data; no holdout to pick \"best\"\n",
    "    eval_strategy=IntervalStrategy.NO, # disable eval during training\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=final_args,\n",
    "    train_dataset=full_tok,     # ALL data used here\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "# 4) Optional: quick reference eval on the small split (not a true test set)\n",
    "ref_eval = Trainer(\n",
    "    model=final_trainer.model,\n",
    "    args=final_args,\n",
    "    eval_dataset=val_small,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ").evaluate()\n",
    "print(\"Reference eval on small holdout:\", ref_eval)\n",
    "\n",
    "# 5) Save\n",
    "final_trainer.model.save_pretrained(\"distilbert-5cls-final/model\")\n",
    "tokenizer.save_pretrained(\"distilbert-5cls-final/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531ddb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('distilbert-5cls-final/model\\\\tokenizer_config.json',\n",
       " 'distilbert-5cls-final/model\\\\special_tokens_map.json',\n",
       " 'distilbert-5cls-final/model\\\\vocab.txt',\n",
       " 'distilbert-5cls-final/model\\\\added_tokens.json',\n",
       " 'distilbert-5cls-final/model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_trainer.model.save_pretrained(\"distilbert-5cls-final/model\")\n",
    "tokenizer.save_pretrained(\"distilbert-5cls-final/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78d5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "import numpy as np, evaluate\n",
    "\n",
    "model_path = \"distilbert-5cls-final/model\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e99e29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "266faef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_15260\\948968231.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ref_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25322' max='25322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25322/25322 02:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model performance:\n",
      "eval_loss            : 1.0687\n",
      "eval_model_preparation_time : 0.0010\n",
      "eval_accuracy        : 0.6273\n",
      "eval_f1_macro        : 0.4519\n",
      "eval_f1_weighted     : 0.6153\n",
      "eval_runtime         : 151.1461\n",
      "eval_samples_per_second : 1340.2590\n",
      "eval_steps_per_second : 167.5330\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "ref_trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=val_small,  # this was your 10% holdout\n",
    ")\n",
    "\n",
    "results = ref_trainer.evaluate()\n",
    "print(\"Final model performance:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:<20} : {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38070dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Austin\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221943e31dd54a3b8def54f145e27cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/2025749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459eee411ae54df3b3b518cd2326d44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/2025749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bd2d7093dc45bebb7b8f1e097acef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1823174 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dae0b2ab9a4aff80ce18ee50a0e0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/202575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449855406f9d44fc9bd22828e7c2f4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2025749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_25496\\3135112512.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='452357' max='455796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [452357/455796 28:21:02 < 12:55, 4.43 it/s, Epoch 3.97/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.827300</td>\n",
       "      <td>0.844383</td>\n",
       "      <td>0.692168</td>\n",
       "      <td>0.470902</td>\n",
       "      <td>0.646640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.829500</td>\n",
       "      <td>0.839747</td>\n",
       "      <td>0.693486</td>\n",
       "      <td>0.492865</td>\n",
       "      <td>0.666408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.841420</td>\n",
       "      <td>0.694572</td>\n",
       "      <td>0.499555</td>\n",
       "      <td>0.667211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, torch, evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "\n",
    "# ==== 1. Load & clean ====\n",
    "df = pd.read_csv(\"TopGamesDataClean.csv\", usecols=[\"content\", \"score\", \"game_name\"])\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df = df[df[\"score\"].between(1,5)].dropna(subset=[\"content\"])\n",
    "df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "df = df[df[\"content\"] != \"\"]\n",
    "\n",
    "# 15  04\n",
    "label_map = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "df[\"label\"] = df[\"score\"].map(label_map).astype(int)\n",
    "\n",
    "# ==== 2. Model / tokenizer ====\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "num_labels = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"content\"], truncation=True, max_length=256)\n",
    "\n",
    "# ==== 3. Dataset + stratified split ====\n",
    "full_ds = Dataset.from_pandas(df[[\"content\",\"label\"]].reset_index(drop=True))\n",
    "full_ds = full_ds.class_encode_column(\"label\")\n",
    "splits    = full_ds.train_test_split(test_size=0.1, stratify_by_column=\"label\", seed=42)\n",
    "train_all = splits[\"train\"]\n",
    "val_small = splits[\"test\"]\n",
    "\n",
    "train_tok = train_all.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "val_tok   = val_small.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "full_tok  = full_ds.map(tokenize, batched=True, remove_columns=[\"content\"])\n",
    "\n",
    "# ==== 4. Data collator & metrics ====\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# ==== 5. Training arguments ====\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"deberta-5cls\",\n",
    "    eval_strategy=IntervalStrategy.EPOCH,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# ==== 6. Trainer ====\n",
    "trainer = Trainer(\n",
    "    model_init=lambda: AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels),\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==== 7. Evaluate on validation set ====\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nValidation results:\")\n",
    "for k, v in eval_results.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k:<20} : {v:.4f}\")\n",
    "\n",
    "# ==== 8. Retrain on full data (optional final model) ====\n",
    "final_args = TrainingArguments(\n",
    "    output_dir=\"deberta-5cls-final\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    evaluation_strategy=IntervalStrategy.NO,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model_init=lambda: AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels),\n",
    "    args=final_args,\n",
    "    train_dataset=full_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "# ==== 9. Save model ====\n",
    "final_trainer.model.save_pretrained(\"deberta-5cls-final/model\")\n",
    "tokenizer.save_pretrained(\"deberta-5cls-final/model\")\n",
    "print(\" Model saved at deberta-5cls-final/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35333c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
